{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "102flower_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ares97/BlockRush/blob/master/102flower_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_UOvD0Fmwu68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "c86bbdcd-20cc-4a26-b179-7a72b3790c80"
      },
      "cell_type": "code",
      "source": [
        "# download PyTorch and other used dependencies\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "!pip install --no-cache-dir -I pillow\n",
        "!pip install Pillow==4.1.1\n",
        "!pip install PIL\n",
        "!pip install image\n",
        "\n",
        "!conda install -c menpo wget --yes\n",
        "!sudo apt install wget\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 2.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.4.1\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.4.1\n",
            "    Uninstalling Pillow-5.4.1:\n",
            "      Successfully uninstalled Pillow-5.4.1\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.7)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n",
            "/bin/bash: conda: command not found\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.19.4-1ubuntu2.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nZKdkjoUxRyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download helper for showing tensors as img etc.\n",
        "!wget -cq https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-to-pytorch/helper.py\n",
        "# Download and unzip data\n",
        "!rm -rf flower_data\n",
        "!wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json\n",
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "!unzip -qq flower_data.zip\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jJllNzGykk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a92adf0-02be-4298-8fcd-bcee485f5870"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    device = 'cpu'\n",
        "else:\n",
        "    device = 'cuda'\n",
        "\n",
        "print(\"Training on\", device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7AAAkqGDy1uz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare and augument the data\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import json\n",
        "\n",
        "\n",
        "data_dir = \"flower_data\"\n",
        "\n",
        "training_transforms = transforms.Compose([transforms.RandomRotation(45),                                                                                                             \n",
        "                                          transforms.RandomHorizontalFlip(),                                               \n",
        "                                          transforms.RandomCrop(224),\n",
        "                                          transforms.RandomResizedCrop(224), \n",
        "                                          transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                               (0.229,0.224,0.225))])    \n",
        "                                                                                                                                         \n",
        "                                                                                                                         \n",
        "validation_transforms = transforms.Compose([transforms.Resize(256), \n",
        "                                            transforms.CenterCrop(224), \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.485,0.456,0.406),\n",
        "                                                                 (0.229,0.224,0.225))])\n",
        "\n",
        "\n",
        "training_dataset = datasets.ImageFolder(data_dir + '/train', transform=training_transforms)\n",
        "validation_dataset = datasets.ImageFolder(data_dir + '/valid', transform=validation_transforms)\n",
        "\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64)\n",
        "\n",
        "### Load label mapping file\n",
        "cat_to_name = json.load(open('cat_to_name.json'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5PDI21w06f4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8e2af97-6383-4406-b5c7-0063bee04ceb"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility function for computing output of convolutions\n",
        "takes a tuple of (h,w) and returns a tuple of (h,w)\n",
        "\"\"\"\n",
        "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
        "    from math import floor\n",
        "    if type(kernel_size) is not tuple:\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
        "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
        "    return h, w\n",
        "  \n",
        "conv_output_shape([224,224], 8, 2, 3)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112, 112)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "qLKwaDfx3MH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=8, stride=2, padding=3, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=3, bias=False) \n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=2, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        self.fc1 = nn.Linear(5*5*64, 1024)\n",
        "        self.fc4 = nn.Linear(1024, 512)\n",
        "        self.fc5 = nn.Linear(512, 102)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "  def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = self.pool(F.relu(self.conv1(x))) # 56x56\n",
        "        x = self.pool(F.relu(self.conv2(x))) # 15x15\n",
        "        x = F.relu(self.conv3(x))            # 8x8\n",
        "        x = F.relu(self.conv4(x))            # 5x5 \n",
        "   #     x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 5*5*64)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.log_softmax(self.fc5(x), dim=1)\n",
        "  \n",
        "        return x\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gYia3OG03fvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f07b4df2-67d3-4102-97da-7aa5fde7798a"
      },
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "\n",
        "model = Net()\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (conv4): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1600, out_features=1024, bias=True)\n",
              "  (fc4): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc5): Linear(in_features=512, out_features=102, bias=True)\n",
              "  (dropout): Dropout(p=0.25)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "-woZi2zI3xoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "2a48b192-9950-41b0-fa2e-6bbf8c019bc1"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "epochs = 20\n",
        "best_acc = np.NINF\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  #set model for training\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  \n",
        "  # training loop\n",
        "  for inputs, labels in training_dataloader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    \n",
        "    # train flow\n",
        "    optimizer.zero_grad()\n",
        "    logps = model.forward(inputs)\n",
        "    loss = criterion(logps, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # calculate train loss and acc\n",
        "    ps = torch.exp(logps)\n",
        "    top_ps, top_class = ps.topk(1, dim=1)\n",
        "    equals = top_class == labels.view(*top_class.shape)\n",
        "    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "    train_loss += loss.item()\n",
        "    \n",
        "    # calculate avg loss and acc and append them to lists\n",
        "    train_losses.append(train_loss / len(training_dataloader))\n",
        "    train_accs.append(train_acc / len(training_dataloader))\n",
        "    \n",
        "\n",
        "    \n",
        "  # set model for validation\n",
        "  model.eval()\n",
        "  valid_acc = 0\n",
        "  valid_loss = 0\n",
        " \n",
        "  # validation loop\n",
        "  for inputs, labels in validation_dataloader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    \n",
        "    # calculate validation loss\n",
        "    logps = model.forward(inputs)\n",
        "    loss = criterion(logps, labels)\n",
        "    valid_loss += loss.item()\n",
        "    \n",
        "    # calculate validation acc\n",
        "    ps = torch.exp(logps)\n",
        "    top_ps, top_class = ps.topk(1, dim=1)\n",
        "    equals = top_class == labels.view(*top_class.shape)\n",
        "    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "  \n",
        "  \n",
        "  \n",
        "  # calculate avg loss and acc and append them to lists\n",
        "  valid_losses.append(valid_loss / len(validation_dataloader))\n",
        "  valid_accs.append(accuracy / len(validation_dataloader))\n",
        "  \n",
        "  print(\"Epoch {}/{}.. \".format(epoch+1, epochs),\n",
        "          \"Training loss: {:.3f}.. \".format(train_losses[epoch]),  \n",
        "          \"Training acc: {:.3f}.. \".format(train_acc[epoch]),  \n",
        "          \"Validation loss: {:.3f}..\".format(valid_loss[epoch]),\n",
        "          \"Validation accuracy: {:.3f}..\".format(valid_acc[epoch]))\n",
        "  \n",
        "  \n",
        "  if accuracy > best_acc:\n",
        "    best_acc = accuracy / len(validation_dataloader)\n",
        "    torch.save(model.state_dict(), 'best_acc_model.pth')\n",
        "    print(\"better acc: {:.3f} -> {:.3f}\".format(best_acc, valid_accs[epoch]))\n",
        "  \n",
        "  "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-43e447015a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# calculate train loss and acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtop_ps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mequals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtop_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eVu1BIBH8IRp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}